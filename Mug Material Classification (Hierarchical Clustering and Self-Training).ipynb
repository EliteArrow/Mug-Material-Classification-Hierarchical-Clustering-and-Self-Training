{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b47f1c0",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "Consider an unlabeled version of the problem where you had data from 3 types of mugs with their height, diameter, weight, and hue (color). (To obtain the unlabeled data, simply assume that the labels are not part of the dataset).\n",
    "\n",
    "a) Implement hierarchical agglomerative clustering for this dataset. Your implementation should provide the option to use single (minimum) linkage, and complete (maximum) linkage.\n",
    "\n",
    "b) Apply your hierarchical clustering implementation with single (minimum) linkage to the data. Choosing the case of 2, 4, 6, and 8 clusters, evaluate to what degree the clusters reflect the type of mug (i.e. the material of the mug) by evaluating the purity of the clusters using the original data labels. For this, compute the accuracy for each cluster assuming that the predicted label for each cluster is the most frequently occurring label in this cluster. Once you computed this for each cluster, compute the overall accuracy using the corresponding number of clusters as the weighted sum of the accuracies where the weight is the fraction of data points in the corresponding cluster.\n",
    "\n",
    "c) Repeat the clustering and evaluation from part 1b) using complete (maximum) linkage. Show the results and discuss any differences you see between single linkage and maximum linkage results (in particular, which one seems to better reflect the material types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91361c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8d767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate Euclidean distance\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((np.array(point1) - np.array(point2)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c053eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate the distance matrix\n",
    "def calculate_distance_matrix(data):\n",
    "    distance_matrix = np.zeros((len(data), len(data)))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i+1, len(data)):\n",
    "            distance = euclidean_distance(data[i], data[j])\n",
    "            distance_matrix[i][j] = distance_matrix[j][i] = distance\n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174f87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find closest clusters\n",
    "def find_closest_clusters(distance_matrix):\n",
    "    min_val = np.inf\n",
    "    x = y = 0\n",
    "    for i in range(len(distance_matrix)):\n",
    "        for j in range(i+1, len(distance_matrix)):\n",
    "            if distance_matrix[i][j] < min_val:\n",
    "                min_val = distance_matrix[i][j]\n",
    "                x, y = i, j\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e38b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to update distance matrix after merging clusters\n",
    "def update_distance_matrix(distance_matrix, x, y, linkage):\n",
    "    row = []\n",
    "    for i in range(len(distance_matrix)):\n",
    "        if i != x and i != y:\n",
    "            if linkage == 'single':\n",
    "                distance = min(distance_matrix[x][i], distance_matrix[y][i])\n",
    "            elif linkage == 'complete':\n",
    "                distance = max(distance_matrix[x][i], distance_matrix[y][i])\n",
    "            row.append(distance)\n",
    "    distance_matrix = np.delete(distance_matrix, [x, y], axis=0)\n",
    "    distance_matrix = np.delete(distance_matrix, [x, y], axis=1)\n",
    "    distance_matrix = np.vstack([distance_matrix, row])\n",
    "    row.append(0)\n",
    "    distance_matrix = np.column_stack([distance_matrix, row])\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c0b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to merge clusters\n",
    "def merge_clusters(clusters, x, y):\n",
    "    clusters[x] += clusters[y]\n",
    "    clusters.pop(y)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27bcddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Agglomerative Clustering\n",
    "def agglomerative_clustering(data, linkage, no_of_clusters):\n",
    "    distance_matrix = calculate_distance_matrix(data)\n",
    "    clusters = [[i] for i in range(len(data))]\n",
    "    while len(clusters) > no_of_clusters:\n",
    "        x, y = find_closest_clusters(distance_matrix)\n",
    "        distance_matrix = update_distance_matrix(distance_matrix, x, y, linkage)\n",
    "        clusters = merge_clusters(clusters, x, y)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc95dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate cluster purity\n",
    "def calculate_purity(clusters, labels):\n",
    "    total_correct_predictions = 0\n",
    "    for cluster in clusters:\n",
    "        cluster_labels = [labels[i] for i in cluster]\n",
    "        most_common = Counter(cluster_labels).most_common(1)[0][0]\n",
    "        correct_predictions = cluster_labels.count(most_common)\n",
    "        total_correct_predictions += correct_predictions\n",
    "    return total_correct_predictions / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70fb7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "with open('dataset.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    headers = next(reader)\n",
    "    data = []\n",
    "    labels = []\n",
    "    for row in reader:\n",
    "        data.append(list(map(float, row[:-1])))\n",
    "        labels.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fa36cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linkage: single, Number of clusters: 2\n",
      "Purity: 0.43333333333333335\n",
      "Linkage: complete, Number of clusters: 2\n",
      "Purity: 0.43333333333333335\n",
      "Linkage: single, Number of clusters: 4\n",
      "Purity: 0.43333333333333335\n",
      "Linkage: complete, Number of clusters: 4\n",
      "Purity: 0.44166666666666665\n",
      "Linkage: single, Number of clusters: 6\n",
      "Purity: 0.4583333333333333\n",
      "Linkage: complete, Number of clusters: 6\n",
      "Purity: 0.45\n",
      "Linkage: single, Number of clusters: 8\n",
      "Purity: 0.475\n",
      "Linkage: complete, Number of clusters: 8\n",
      "Purity: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the data\n",
    "data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n",
    "\n",
    "# Applying the algorithm\n",
    "for no_of_clusters in [2, 4, 6, 8]:\n",
    "    for linkage in ['single', 'complete']:\n",
    "        print(f\"Linkage: {linkage}, Number of clusters: {no_of_clusters}\")\n",
    "        clusters = agglomerative_clustering(data, linkage, no_of_clusters)\n",
    "        purity = calculate_purity(clusters, labels)\n",
    "        print(f\"Purity: {purity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28586daf",
   "metadata": {},
   "source": [
    "It's difficult to definitively say which method is better at reflecting the material types of the mugs. The results depend on the chosen number of clusters. At a lower number of clusters, complete linkage seems to perform slightly better, while at a higher number of clusters, single linkage seems to perform better.\n",
    "\n",
    "The differences in purity scores are relatively small, and the overall purity scores are not particularly high, which suggests that neither method is able to perfectly capture the underlying classes in the data. This might be due to the complexity of the data, noise, or other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e2dfc",
   "metadata": {},
   "source": [
    "### Self-Training\n",
    "\n",
    "Take the first 10 data items from each class as the supervised data, Ds, (keeping its labels) and the remaining data items as unsupervised data, Du, (by ignoring its labels).\n",
    "\n",
    "a) Implement a self-training system using a K-nearest neighbor classifier for this problem (to evaluate the level of certainty for each prediction on unlabeled data you can use the purity of the prediction - i.e. how many more ”votes” the predicted class received than the other class - or you can use distance weighted voting where the number of votes each of the K data points receives is inversely proportional to the distance from the data point). Your algorithm should allow you to change the number of data items that are being added to the labeled data set in each iteration.\n",
    "\n",
    "b) Learn a classifier using the semi-supervised learning algorithm and compare it agains the K-nearest neighbor classifier learned only from the labeled data Ds by evaluating the fraction of initially unlabeled points it predicts correctly (for the self-learning you can simply use the fraction of the initially unlabeled points that were assigned the correct label during the learning process). Repea the comparison for four different numbers of data points added in each iteration, including adding 1 data point in each iteration, adding all data points in the first iteration, adding 5 points in each iteration, and one setting of your choice. Discuss if you see any performance difference and if so, what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa7fe740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a08063c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf8eb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(X_train, test_row, k):\n",
    "    distances = []\n",
    "    for train_row in X_train:\n",
    "        dist = euclidean_distance(test_row, train_row[:-1])\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = [tup[0] for tup in distances[:k]]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b42c8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification(X_train, test_row, k, threshold):\n",
    "    neighbors = get_neighbors(X_train, test_row, k)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction, count = Counter(output_values).most_common(1)[0]\n",
    "    if len(Counter(output_values)) > 1:\n",
    "        _, second_most_common_count = Counter(output_values).most_common(2)[1]\n",
    "        if count / second_most_common_count < threshold:\n",
    "            return None\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a852cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_training(X_train, X_unlabel, k, n, threshold):\n",
    "    while len(X_unlabel) > 0 and n > 0:\n",
    "        X_unlabel_new = np.empty((0, 4))\n",
    "        for i in range(len(X_unlabel)):\n",
    "            if i == n:\n",
    "                break\n",
    "            test_row = X_unlabel[i]\n",
    "            label = predict_classification(X_train, test_row, k, threshold)\n",
    "            if label is not None:\n",
    "                test_row = np.append(test_row, label)\n",
    "                X_train = np.vstack([X_train, test_row])\n",
    "            else:\n",
    "                X_unlabel_new = np.vstack([X_unlabel_new, test_row])\n",
    "        X_unlabel = np.vstack([X_unlabel_new, X_unlabel[n:]])\n",
    "        n -= 1\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160388bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a62888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points added in each iteration: 1\n",
      "Accuracy: 0.71\n",
      "Number of points added in each iteration: 100\n",
      "Accuracy: 0.75\n",
      "Number of points added in each iteration: 5\n",
      "Accuracy: 0.76\n",
      "Number of points added in each iteration: 10\n",
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df['Type'] = pd.Categorical(df['Type'])\n",
    "df['Type'] = df['Type'].cat.codes\n",
    "\n",
    "# Normalize the features\n",
    "for column in df.columns[:-1]:\n",
    "    df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "\n",
    "data = df.values\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Splitting data into labelled and unlabelled\n",
    "X_train = data[:20]\n",
    "X_unlabel = data[20:, :-1]\n",
    "\n",
    "k = 3\n",
    "n = [1, len(X_unlabel), 5, 10]\n",
    "threshold = 2\n",
    "\n",
    "for i in n:\n",
    "    X_train_new = self_training(X_train.copy(), X_unlabel.copy(), k, i, threshold)\n",
    "    y_true = data[20:, -1]\n",
    "    y_pred = []\n",
    "    for row in data[20:]:\n",
    "        prediction = predict_classification(X_train_new, row[:-1], k, threshold)\n",
    "        if prediction is not None:\n",
    "            y_pred.append(prediction)\n",
    "        else:\n",
    "            y_pred.append(-1)  # mark the data points that couldn't be classified\n",
    "    print(\"Number of points added in each iteration:\", i)\n",
    "    print(\"Accuracy:\", accuracy(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0f6ab",
   "metadata": {},
   "source": [
    "These results show that the accuracy of the classifier improves when more data points are added in each iteration. This is to be expected, as adding more data points in each iteration allows the classifier to learn from a larger portion of the data set at each step, which can help it make better predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
